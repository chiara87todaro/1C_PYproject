# A report on sales forecasting
###### A Kaggle project

#### Synopsis
Sales forecasting is a common problem that can be optimized by 
using Machine Learning techniques.
In this report, sales records made by the Russian software company 
*1C Company* have been analysed. 
Based on past sales, the amount of items that will be sold in a 
certain shop is predicted. 
The forecast accuracy is 96.06% with an error of +/- 5 units.
------------------------------------------------------

## Data

1C Company is an independent software developer, distributor and 
publisher based in Moscow, Russia. It deals with development, licence, 
support, and sale of computer software and video games. 
The company operates through a wide network of more than 10000 
business partners spread across 25 countries. In 2006 the trademark 
"1C" has been acknowledge wide popularity by the Russian Federal 
Service for Intellectual Property <sup>[1]</sup>.
 
[1]: [link](https://en.wikipedia.org/wiki/1C_Company).


```{python import_library,echo=False,message=False,warning=False,cache=True}
import os
import sys
import numpy as np # scientific calculation
import pandas as pd # data analysis
import matplotlib.pyplot as plt # data plot
import seaborn as sns # data plot 
import statsmodels.api as sm
#import networkx as nx
from sklearn.ensemble import RandomForestRegressor

import chart_studio.plotly as py
import plotly.graph_objects as go
from plotly.offline import plot
from plotly.subplots import make_subplots

basicPath="/home/chiara/kaggle/1C_PYproject/" #scripts/
os.chdir(basicPath+"scripts/")
sys.path.append(os.getcwd())
import my_functions_1c as ct 
```

In here, a small record of products sold in Russian shops has been analysed.
Data consist in almost 3 million observations: each one indicates **how many 
items** have been sold in a certain **day**, in a certain **shop**, at a 
certain **price**. An incremental index is assigned to each month: sales 
have been recorded from January 2013 to October 2015.
Additional data consist in pair-wise dictionaries with the actual name and 
the id of items, shops, and categories, respectively. 
The original data set has been provided by Kaggle platform and can be found 
[here](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data).

### Data summary

The aim of this report is to give a reliable estimate of the monthly 
sales of 1C stores. Different stores can have different needs, and 
certaintly, different items are sold more than others.
Before engaging the prediction algorithm, let's take a closer look 
at the features that could influence the estimate the most. 
First, the data set is splitted into two parts: 

- **control data set**: from Jan 2013 to Dec 2013, the oldest sales, 
used to test the prediction algorithm
- **training data set**: from Jan 2014 to Oct 2015, the most recent 
sales, used to build the prediction algorithm 

```{python plot_dataAll,echo=False,cache=True,result="hide"}
filePath=basicPath +"scripts/working_data/"+"1C_small_training.csv"
data=pd.read_csv(filePath, index_col=False) 
data=data.drop("Unnamed: 0",axis=1)

filePath=basicPath+"scripts/working_data/"+"1C_ctrl_training.csv"
data2=pd.read_csv(filePath, index_col=False) 
data2=data2.drop(["Unnamed: 0",'Unnamed: 0.1', 'Unnamed: 0.1.1'],axis=1)

dataAll=data2.append(data)

aN_itemID=dataAll["item_id"].value_counts().reset_index().sort_values("index")
aN_itemPrice=dataAll["item_price"].value_counts().reset_index().sort_values("index")
aN_categoryID=dataAll["item_category_id"].value_counts().reset_index().sort_values("index")
aN_shopID=dataAll["shop_id"].value_counts().reset_index().sort_values("index")

N_itemID=data["item_id"].value_counts().reset_index().sort_values("index")
N_itemPrice=data["item_price"].value_counts().reset_index().sort_values("index")
N_categoryID=data["item_category_id"].value_counts().reset_index().sort_values("index")
N_shopID=data["shop_id"].value_counts().reset_index().sort_values("index")

```

There is quite a big range of products and stores: <%= len(N_shopID) %> shops 
sold <%= len(N_itemID) %> items divided into <%= len(N_categoryID) %> 
categories which span a price range of <%= abs(N_itemPrice["index"]).min() %> - 
<%= N_itemPrice["index"].max() %> Russian Ruples.
Note that in the training data are "missing" <%= -len(N_itemID)+len(aN_itemID)%> 
items, <%= -len(N_shopID)+len(aN_shopID) %> shops, and 
<%= -len(N_categoryID) +len(aN_categoryID)%> categories from the control data set.
This is not a problem, indeed it allows to see how the algorithm 
behaves on unknown data. In order to summarize important features of the data, 
some summary figures are shown.


The upper panel of Figure 1 shows the total number of item sold in each shop 
(black lines represent the variability over items). 
The lower panel of Figure 1 shows that the most productive shop is the 
Moscow shopping center "Semenovsky" (31) with more than 175 thousand of items 
sold, followed by the Moscow mall "Atrium" (25) with more than 135 thousand 
sales, and the shopping mall "MEGA Tepliy Stan" (28) in the outskirt of
 Moscow with more then 100 thousand sales. The less productive shops are 
 the shop "Zhukovsky" (11), and "Novosibirsk"" (36) shopping and 
 entertainment center in Middle Russia.

```{python fig_cntXshop,echo=False,cache=False,result="hide",caption = 'Figure 1',fig=True,width="900px"}

# tot num of item_id sold in each shop
data_shop_sales=data.groupby(["shop_id","item_id"])["item_cnt_day"].sum().reset_index()

data=ct.my_createMonthYear(data)
# tot num of items sold in each shop each month
data_shop_trend=data.groupby(["date_block_num","MonYe","shop_id"])["item_cnt_day"].sum().reset_index()#
DB3=data_shop_trend.sort_values("item_cnt_day",ascending=False)
popular_shop=DB3["shop_id"].head(n=100).unique()

DB3=data_shop_trend[data_shop_trend.shop_id.isin(popular_shop)]


sns.set(font_scale=1.5,rc={'figure.figsize':(25,15)})
fig3, ax3 = plt.subplots(2, 1)
sns.barplot(x="shop_id",y="item_cnt_day",estimator=sum,data=data_shop_sales,ax=ax3[0])
sns.pointplot(x='MonYe', y='item_cnt_day',hue='shop_id',data=DB3,ax=ax3[1])
#ax3[0].set_xticklabels(ax3[0].get_xticklabels(), rotation=45)
ax3[1].set_xticklabels(ax3[1].get_xticklabels(), rotation=45)
ax3[0].set_title("Number of items sold in each shop")
ax3[1].set_title("Popular shops trends")
fig3
```


The upper panel of Figure 2 shows the number of items in each category. 
Category 40 is the wider and includes more than 3500 movies in DVD; 
categories 55 and 37 collects more than 1500 of items each, representing 
BLU-RAYS movies and local music CDs, respectively.
The lower panel of Figure 2 shows sale trends of the most popular categories. 
The total number of items, for each month and for each category, has 
been calculated and plotted.
Note that all categories have a peak on December, likely due to 
festivities. 
*The most sold items are DVDs (40), PC games (30), and local music 
CDs (55).*
More unpopular categories includes special editions games for PC (28) 
and games for XBOX 360 (23).
Also, note that the most popular categories have a steeper descending 
trend in respect to the unpopular ones, which have more stable sales.

```{python fig_catTrend,echo=False,cache=False,message=False,caption = 'Figure 2',fig=True,width="900px"}
data_categ=data.groupby(["item_category_id"])["item_id"].unique()
# number of items for each category
DD1=data_categ.apply(lambda x:len(x)).reset_index()
DD1.columns=["item_category_id","num_items"]


data=ct.my_createMonthYear(data)
data_category_trend=data.groupby(["date_block_num","MonYe","item_category_id"])["item_cnt_day"].sum().reset_index()#
DB1=data_category_trend.sort_values("item_cnt_day",ascending=False)
popular_cat=DB1["item_category_id"].head(n=100).unique()

DB1=data_category_trend[data_category_trend.item_category_id.isin(popular_cat)]

sns.set(font_scale=1.5,rc={'figure.figsize':(25,15)})
fig1, ax1 = plt.subplots(2, 1)
sns.barplot(x="item_category_id",y="num_items",data=DD1,ax=ax1[0])
sns.pointplot(x='MonYe',y='item_cnt_day',hue='item_category_id',data=DB1,ax=ax1[1])
ax1[0].set_title("Number of items per category")
ax1[1].set_title("Popular categories trends")
#ax1[0].set_xticklabels(ax1[0].get_xticklabels(), rotation=45)
ax1[1].set_xticklabels(ax1[1].get_xticklabels(), rotation=45)
fig1
```

The left panel of Figure 3 shows in blue the price range for each category.
For example, the price of DVD (category=40) ranges from 10 to less 
than 5000 Ruples, which correspond to 0.14-70 Euros; 
a PC game (30) is sold up to 5000 Ruples (70 Euros).
The right panel of Figure 3 shows how many items, on average, 
have been sold for each price range. 
The most popular items cost between 100 and 499 Ruples with 40 
thousands units sold, followed by items with a price up to 5000 
Ruples. 
*Medium priced items strongly contribute to the total sales, while 
cheap or very expensive items are less relevant.*


```{python fig_catPrice,echo=False,cache=False,message=False,caption = 'Figure 3',fig=True,width="900px"}
price_ranges=[0,10,50,100,500,1000,5000,10000,50000,310000]
price_labs=["<10","<50","<100","<500","<1000","<5000","<10000","<50000","<310000"]
data_price_sales=data.groupby(["item_id","item_price","date_block_num","item_category_id"])["item_cnt_day"].sum().reset_index()
data_price_sales["price_range"]=pd.cut(data_price_sales["item_price"],bins = price_ranges,labels = price_labs)
data_price_sales=data_price_sales.sort_values("date_block_num")

data_price_sales11=pd.pivot_table(data_price_sales, #[["item_category_id","price_range"]]
                                   values="item_price",index=["item_category_id"],
                                   columns=["price_range"])
AA=data_price_sales11.notna()
data_price_sales2=data_price_sales.groupby(["date_block_num","price_range"])["item_cnt_day"].sum().reset_index()

#stripplot swarmplot
sns.set(font_scale=1.5,rc={'figure.figsize':(25,15)})
fig2, ax2 = plt.subplots(1, 2)
sns.heatmap(AA, cmap="YlGnBu",ax=ax2[0])   #,center=0.5
# sns.stripplot(y="item_category_id", x="price_range",data=B,ax=ax2[0])#
sns.barplot(x="price_range", y="item_cnt_day",data=data_price_sales2,ax=ax2[1])
ax2[0].set_xticklabels(ax2[0].get_xticklabels(), rotation=45)
ax2[1].set_xticklabels(ax2[1].get_xticklabels(), rotation=45)
fig2
```

In addition to price and category, another feature that can help 
forecast sales is the average past sales. For each month, 
for each shop, and for each item the number of units sold till 
that moment (starting from Jan 14) are averaged.
The average past sales can also be used as a rough forecast to 
evaluate the result of the machine learning algorithm. 


## Forecast

```{python training_err,echo=False,message=False,error=False}
dfPath=basicPath+"scripts/working_data/"+"1C_train_err.csv"
train_err=pd.read_csv(dfPath, index_col=False) 
dfPath=basicPath+"scripts/working_data/"+"1C_ctrlAStest_err.csv"
ctrl_err=pd.read_csv(dfPath, index_col=False) 

rmseTrain=np.zeros(3)
accTrain=np.zeros(3)
thr=[1,2,6]
for i in range(3):
   [rmseTrain[i],accTrain[i]]=ct.my_calculate_RMSE_ACC(train_err,"actual",
                                              predictions=train_err["predicted"],
                                              fitModel=None,
                                              make_pred=False,thres=thr[i])

rmseCtrl=np.zeros(3)
accCtrl=np.zeros(3)
thr=[1,2,6]
for i in range(3): 
    [rmseCtrl[i],accCtrl[i]]=ct.my_calculate_RMSE_ACC(ctrl_err,"actual",
                               predictions=ctrl_err["predicted"],
                               fitModel=None,make_pred=False,thres=thr[i])

```

The Random Forest method has been chosen to forecast the monthly sales 
for each item and shop of 1C company. The algorithm has been trained 
on the most recent sales, from Jan 2014 to Oct 2015.
The predicted amount of items sold during the training period 
(Jan 14- Oct 15) exactly matches the true amount of sales 
<%= round(accTrain[0],ndigits=2) %>% of the times. However, allowing a 
margin of error of just 1 unit, the accuracy increses to 
<%= round(accTrain[1],ndigits=2) %>%, and within +/- 5 units the accuracy 
reaches <%= round(accTrain[2],ndigits=2) %>%.

To perform a reliable test of the algorithm, the true amount of items sold 
from Jan 2013 to Dec 2013 has been discarded. The average past sales have 
been calculated by copying the ones of the training data set. For new items 
and shops, the average past sales is set to 0.
The forecast for the "control" sales is exactly accurate at 
<%= round(accCtrl[0],ndigits=2) %>%. By allowing an error within +/- 1 
and +/- 5 units, the accuracy increases to <%= round(accCtrl[1],ndigits=2) %>% 
and <%= round(accCtrl[2],ndigits=2) %>%, respectively. 



```{python comp_pred,echo=False,cache=False,message=False,caption = 'Figure 4',fig=True,width="900px"}
dfPath=basicPath+"scripts/working_data/"+"ctrl_dataPredictions.csv"
data_predictions=pd.read_csv(dfPath,index_col=False)

data_predictions=data_predictions.reset_index().set_index(["index","new"])#"actual","predicted","historic"
data_predictions2=data_predictions.stack().reset_index()
data_predictions2
data_predictions2.columns=["id","new","method","sales"]


sns.set(font_scale=2.5,rc={'figure.figsize':(25,15)})
fig4, ax4 = plt.subplots(1, 1)
sns.scatterplot(x="id",y="sales",hue="method",x_jitter=0.2,y_jitter=0.2,
                data=data_predictions2[data_predictions2["id"]<700],ax=ax4)
fig4


rmseHist=np.zeros(3)
accHist=np.zeros(3)
thr=[1,2,6]
for i in range(3):
    [rmseHist[i],accHist[i]]=ct.my_calculate_RMSE_ACC(data_predictions,"actual",
                                   predictions=data_predictions["historic"],
                                   fitModel=None, make_pred=False,thres=thr[i])

```
Figure 4 shows the improvement of the machine learning prediction and the 
estimate using "historic" data for a subset of the control data set. 
The overall accuracy for the historic estimate is <%= round(accHist[0],ndigits=2) %>%, 
<%= round(accHist[1],ndigits=2) %>%, and <%= round(accHist[2],ndigits=2) %>% 
within +/-0, +/-1, and +/-5 items, respectively.



